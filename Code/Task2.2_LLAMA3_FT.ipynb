{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# LLAMA3 Fine-tuning for text classification using QLORA\n",
        "\n",
        "\n",
        "### Requirements:\n",
        "* A GPU with enough memory!\n",
        "\n",
        "### Installs\n",
        "* They suggest using latest version of transformers\n",
        "* Must restart after install because the accelerate package used in the hugging face trainer requires it."
      ],
      "metadata": {
        "id": "IqufrL0vwDod"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3zK5QqrIkqif",
        "outputId": "97f2dd7e-6b55-428f-d0c4-c2f47607cc92"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QzSvk9-psdeH",
        "outputId": "c299c848-c1ff-47f9-937a-11c2e0cc4434",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.2.2\n",
            "  Downloading torch-2.2.2-cp310-cp310-manylinux1_x86_64.whl (755.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.5/755.5 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (2.15.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.2.2) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.2.2)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.2.2)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.2.2)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.2.2)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.2.2)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.2.2)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.19.3 (from torch==2.2.2)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.2.2)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.2.0 (from torch==2.2.2)\n",
            "  Downloading triton-2.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (167.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.2.2)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m68.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.6)\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.25.2)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard) (3.0.3)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.2.2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard) (3.2.2)\n",
            "Installing collected packages: triton, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.2.2 which is incompatible.\n",
            "torchvision 0.18.0+cu121 requires torch==2.3.0, but you have torch 2.2.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 torch-2.2.2 triton-2.2.0\n",
            "Collecting transformers==4.40.0\n",
            "  Downloading transformers-4.40.0-py3-none-any.whl (9.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.0/9.0 MB\u001b[0m \u001b[31m67.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting datasets==2.18.0\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate==0.29.3\n",
            "  Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m297.6/297.6 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting evaluate==0.4.1\n",
            "  Downloading evaluate-0.4.1-py3-none-any.whl (84 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes==0.43.1\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting huggingface_hub==0.22.2\n",
            "  Downloading huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m388.9/388.9 kB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting trl==0.8.6\n",
            "  Downloading trl-0.8.6-py3-none-any.whl (245 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m245.2/245.2 kB\u001b[0m \u001b[31m35.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting peft==0.10.0\n",
            "  Downloading peft-0.10.0-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.1/199.1 kB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (3.14.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.40.0) (4.66.4)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets==2.18.0)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (2.0.3)\n",
            "Collecting xxhash (from datasets==2.18.0)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m819.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets==2.18.0)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (3.9.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.29.3) (2.2.2)\n",
            "Collecting responses<0.19 (from evaluate==0.4.1)\n",
            "  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub==0.22.2) (4.12.1)\n",
            "Collecting tyro>=0.5.11 (from trl==0.8.6)\n",
            "  Downloading tyro-0.8.4-py3-none-any.whl (102 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.4/102.4 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.40.0) (2024.6.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (3.1.4)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.29.3) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.29.3) (12.5.40)\n",
            "Requirement already satisfied: docstring-parser>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.8.6) (0.16)\n",
            "Requirement already satisfied: rich>=11.1.0 in /usr/local/lib/python3.10/dist-packages (from tyro>=0.5.11->trl==0.8.6) (13.7.1)\n",
            "Collecting shtab>=1.5.6 (from tyro>=0.5.11->trl==0.8.6)\n",
            "  Downloading shtab-1.7.1-py3-none-any.whl (14 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (2.16.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.29.3) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.29.3) (1.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=11.1.0->tyro>=0.5.11->trl==0.8.6) (0.1.2)\n",
            "Installing collected packages: xxhash, shtab, dill, responses, multiprocess, huggingface_hub, tyro, transformers, datasets, bitsandbytes, accelerate, trl, peft, evaluate\n",
            "  Attempting uninstall: huggingface_hub\n",
            "    Found existing installation: huggingface-hub 0.23.2\n",
            "    Uninstalling huggingface-hub-0.23.2:\n",
            "      Successfully uninstalled huggingface-hub-0.23.2\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "Successfully installed accelerate-0.29.3 bitsandbytes-0.43.1 datasets-2.18.0 dill-0.3.8 evaluate-0.4.1 huggingface_hub-0.22.2 multiprocess-0.70.16 peft-0.10.0 responses-0.18.0 shtab-1.7.1 transformers-4.40.0 trl-0.8.6 tyro-0.8.4 xxhash-3.4.1\n"
          ]
        }
      ],
      "source": [
        "# Install Pytorch\n",
        "%pip install \"torch==2.2.2\" tensorboard\n",
        "\n",
        "# Install Hugging Face libraries\n",
        "%pip install  --upgrade \"transformers==4.40.0\" \"datasets==2.18.0\" \"accelerate==0.29.3\" \"evaluate==0.4.1\" \"bitsandbytes==0.43.1\" \"huggingface_hub==0.22.2\" \"trl==0.8.6\" \"peft==0.10.0\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Big Picture Overview of Parameter Efficient Fine Tuning Methods like LoRA and QLoRA Fine Tuning for Sequence Classification\n",
        "\n",
        "**The Essence of Fine-tuning**\n",
        "- LLMs are pre-trained on vast amounts of data for broad language understanding.\n",
        "- Fine-tuning is crucial for specializing in specific domains or tasks, involving adjustments with smaller, relevant datasets.\n",
        "\n",
        "**Model Fine-tuning with PEFT: Exploring LoRA and QLoRA**\n",
        "- Traditional fine-tuning is resource-intensive; PEFT (Parameter Efficient Fine-tuning) makes the process faster and less demanding.\n",
        "- Focus on two PEFT methods: LoRA and QLoRA.\n",
        "\n",
        "**The Power of PEFT**\n",
        "- PEFT modifies only a subset of the LLM's parameters, enhancing speed and reducing memory demands, making it suitable for less powerful devices.\n",
        "\n",
        "**LoRA: Efficiency through Adapters**\n",
        "- **Low-Rank Adaptation (LoRA):** Injects small trainable adapters into the pre-trained model.\n",
        "- **Equation:** For a weight matrix $W$, LoRA approximates $W = W_0 + BA$, where $W_0$ is the original weight matrix, and $BA$ represents the low-rank modification through trainable matrices $B$ and $A$.\n",
        "- Adapters learn task nuances while keeping the majority of the LLM unchanged, minimizing overhead.\n",
        "\n",
        "**QLoRA: Compression and Speed**\n",
        "- **Quantized LoRA (QLoRA):** Extends LoRA by quantizing the model’s weights, further reducing size and enhancing speed.\n",
        "- **Innovations in QLoRA:**\n",
        "  1. **4-bit Quantization:** Uses a 4-bit data type, NormalFloat (NF4), for optimal weight quantization, drastically reducing memory usage.\n",
        "  2. **Low-Rank Adapters:** Fine-tuned with 16-bit precision to effectively capture task-specific nuances.\n",
        "  3. **Double Quantization:** Reduces quantization constants from 32-bit to 8-bit, saving additional memory without accuracy loss.\n",
        "  4. **Paged Optimizers:** Manages memory efficiently during training, optimizing for large tasks.\n",
        "\n",
        "**Why PEFT Matters**\n",
        "- **Rapid Learning:** Speeds up model adaptation.\n",
        "- **Smaller Footprint:** Eases deployment with reduced model size.\n",
        "- **Edge-Friendly:** Fits better on devices with limited resources, enhancing accessibility.\n",
        "\n",
        "**Conclusion**\n",
        "- PEFT methods like LoRA and QLoRA revolutionize LLM fine-tuning by focusing on efficiency, facilitating faster adaptability, smaller models, and broader device compatibility.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "jIS0yOVNRHcB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning for Sentiment Analysis Classification:\n",
        "\n",
        "\n",
        "#### 1. Text Generation with Sentiment Label as part of text\n",
        "- **Approach**: Train the model to generate text that naturally appends the sentiment label at the end.\n",
        "- **Input**: \"TSLA slashes model Y prices ======\"\n",
        "- **Output**: \"TSLA slashes model Y prices ====== Bearish\"\n",
        "- **Use Case**: This method is useful for applications requiring continuous text output that includes embedded sentiment analysis, such as interactive chatbots or automated content creation tools.\n",
        "\n",
        "\n",
        "#### 2. Sequence Classification Head\n",
        "- **Approach**: Add a sequence classification head (linear layer) on top of the LLaMa Model transformer. This setup is similar to GPT-2 and focuses on classifying the sentiment based on the last relevant token in the sequence.\n",
        "    - **Token Positioning**:\n",
        "        - **With pad_token_id**: The model identifies and ignores padding tokens, using the last non-padding token for classification.\n",
        "        - **Without pad_token_id**: It defaults to the last token in each sequence.\n",
        "        - **inputs_embeds**: If embeddings are directly passed (without input_ids), the model cannot identify padding tokens and takes the last embedding in each sequence as the input for classification.\n",
        "- **Input**: Specific sentences (e.g., \"TSLA slashes Model Y prices\").\n",
        "- **Output**: Direct sentiment classification (e.g., \"Bearish\").\n",
        "- **Training Objective**: Minimize cross-entropy loss between the predicted and the actual sentiment labels.\n",
        "\n",
        "https://huggingface.co/docs/transformers/main/en/model_doc/llama\n",
        "\n",
        "### Peft Configs\n",
        "* Bits and bytes config for quantization\n",
        "* Lora config for lora\n",
        "\n",
        "### Going to use Hugginface Transformers trainer class: Main componenents\n",
        "* Hugging face dataset (for train + eval)\n",
        "* Data collater\n",
        "* Compute Metrics\n",
        "* Class weights since we use custom trainer and also custom weighted loss..\n",
        "* trainingArgs: like # epochs, learning rate, weight decay etc..\n",
        "\n"
      ],
      "metadata": {
        "id": "QAs9tbj8tiBZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zCNt55YNyA3d"
      },
      "source": [
        "### Login to huggingface hub to put your LLama token so we can access Llama 3 7B Param Pre-trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N8157Tsw3Vo3",
        "outputId": "d8203012-58ec-44dd-a5d0-9f69cb5aeeb4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) n\n",
            "Token is valid (permission: write).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "!huggingface-cli login"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Imports"
      ],
      "metadata": {
        "id": "Z_5AVUyey1io"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h5NPLc7isjdM"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import random\n",
        "import functools\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import evaluate\n",
        "\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score, confusion_matrix, classification_report, balanced_accuracy_score, accuracy_score\n",
        "\n",
        "from datasets import Dataset, DatasetDict\n",
        "from peft import LoraConfig, prepare_model_for_kbit_training, get_peft_model\n",
        "\n",
        "from transformers import (\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Youtube Video Describing How to get Dataset\n",
        "* Only really need first 3 mins of video"
      ],
      "metadata": {
        "id": "A5NaDTjvbO5u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import YouTubeVideo\n",
        "YouTubeVideo('ascf3y7zSaY')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "aO711xGybRai",
        "outputId": "68dee731-1d70-451f-8ea7-3561d305722c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.lib.display.YouTubeVideo at 0x7862e640e8f0>"
            ],
            "text/html": [
              "\n",
              "        <iframe\n",
              "            width=\"400\"\n",
              "            height=\"300\"\n",
              "            src=\"https://www.youtube.com/embed/ascf3y7zSaY\"\n",
              "            frameborder=\"0\"\n",
              "            allowfullscreen\n",
              "            \n",
              "        ></iframe>\n",
              "        "
            ],
            "image/jpeg": "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\n"
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPHo5hnA1Fsq"
      },
      "source": [
        "#### Load TSLA sentiment analysis dataset\n",
        "* Derived from Alpha vantage text data...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Add also a numeric 0,1,2 version of label since we will need it later for fine tuning. We can save it in 'target'"
      ],
      "metadata": {
        "id": "tC0ls6RVlzHC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/csvs/task2_train.csv\")\n",
        "df_test = pd.read_csv(\"/content/drive/MyDrive/csvs/task2_dev.csv\")\n",
        "df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "yxy2D8w6u9F4",
        "outputId": "d9ec9723-1f7c-474e-fb65-a844838527af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                  text1  \\\n",
              "0     Japan as a geography for us is a high transact...   \n",
              "1     So, on the op margin side, although there was ...   \n",
              "2     because that's sold through the same Facebook ...   \n",
              "3     They don't require a big network effect or a l...   \n",
              "4     We continue to invest in marketing and adverti...   \n",
              "...                                                 ...   \n",
              "5516  We have 400 million people sharing with Instag...   \n",
              "5517  We do see it; sometimes it is higher than othe...   \n",
              "5518  Most agents and their knowledge or smarts come...   \n",
              "5519  We continue to see and saw again installed bas...   \n",
              "5520              We launched Echo and Alexa in France.   \n",
              "\n",
              "                                                  text2  label  \n",
              "0     The improvement in that in Q3 is obviously ver...      1  \n",
              "1     So we continue to build the underpinnings of t...      1  \n",
              "2     In terms of the development on the advertising...      1  \n",
              "3     And the advantage of that is also that some of...      1  \n",
              "4     We want to continue to invest in the business ...      1  \n",
              "...                                                 ...    ...  \n",
              "5516  So, on Stories, we've seen great progress with...      1  \n",
              "5517  The starting point for our expectation would b...      1  \n",
              "5518  But more importantly, we have a much more broa...      0  \n",
              "5519  And I think we were quite encouraged by E5 per...      0  \n",
              "5520  So I would say that we're continuing to frontl...      1  \n",
              "\n",
              "[5521 rows x 3 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be19d8b5-bfb6-4a43-9723-7dd27f1704df\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text1</th>\n",
              "      <th>text2</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Japan as a geography for us is a high transact...</td>\n",
              "      <td>The improvement in that in Q3 is obviously ver...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>So, on the op margin side, although there was ...</td>\n",
              "      <td>So we continue to build the underpinnings of t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>because that's sold through the same Facebook ...</td>\n",
              "      <td>In terms of the development on the advertising...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>They don't require a big network effect or a l...</td>\n",
              "      <td>And the advantage of that is also that some of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>We continue to invest in marketing and adverti...</td>\n",
              "      <td>We want to continue to invest in the business ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5516</th>\n",
              "      <td>We have 400 million people sharing with Instag...</td>\n",
              "      <td>So, on Stories, we've seen great progress with...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5517</th>\n",
              "      <td>We do see it; sometimes it is higher than othe...</td>\n",
              "      <td>The starting point for our expectation would b...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5518</th>\n",
              "      <td>Most agents and their knowledge or smarts come...</td>\n",
              "      <td>But more importantly, we have a much more broa...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5519</th>\n",
              "      <td>We continue to see and saw again installed bas...</td>\n",
              "      <td>And I think we were quite encouraged by E5 per...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5520</th>\n",
              "      <td>We launched Echo and Alexa in France.</td>\n",
              "      <td>So I would say that we're continuing to frontl...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5521 rows × 3 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be19d8b5-bfb6-4a43-9723-7dd27f1704df')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-be19d8b5-bfb6-4a43-9723-7dd27f1704df button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-be19d8b5-bfb6-4a43-9723-7dd27f1704df');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-21651e02-573e-4e4a-8b16-08d1e4e467f9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-21651e02-573e-4e4a-8b16-08d1e4e467f9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-21651e02-573e-4e4a-8b16-08d1e4e467f9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "  <div id=\"id_5c8553b1-5ea1-41d2-bc83-660cea9d1ccd\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_5c8553b1-5ea1-41d2-bc83-660cea9d1ccd button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 5521,\n  \"fields\": [\n    {\n      \"column\": \"text1\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4841,\n        \"samples\": [\n          \"And that\\u2019s what\\u2019s really driving some of the economics.\",\n          \"And if we can grow demand faster than we grow supply, then we're going to see that play through in price.\",\n          \"We will have IaaS services, and you talked about the existence, proof at least from Amazon about what margins at scale can be achieved there.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"text2\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2866,\n        \"samples\": [\n          \"We then recently came out with ARKit 2.\",\n          \"And so as we work on rolling out more Ad Breaks, and we are rolling out slowly, we're really focused on finding ways to help marketers measure the right things, and that's a very important focus for the company going forward.\",\n          \"The premium services, for example, the way we think about them is in everything related to our data is, and especially the higher level databases, so I'm not talking about raw storage but this is Cosmos DB or Azure DB or any of data services, our IoT services, our AI services are all the premium services.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 2,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['label']=df['label'].astype('category')\n",
        "df['target']=df['label'].cat.codes\n",
        "df['summary']= list(zip(df['text1'],df['text2']))\n",
        "df_test['label']=df_test['label'].astype('category')\n",
        "df_test['target']=df_test['label'].cat.codes\n",
        "df_test['summary']= list(zip(df_test['text1'],df_test['text2']))\n",
        "df.head().summary\n",
        "# df_test.head()"
      ],
      "metadata": {
        "id": "kHiBd07ikx0N",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b233d727-fca3-4fbb-86c5-290a83f89b2e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    (Japan as a geography for us is a high transac...\n",
              "1    (So, on the op margin side, although there was...\n",
              "2    (because that's sold through the same Facebook...\n",
              "3    (They don't require a big network effect or a ...\n",
              "4    (We continue to invest in marketing and advert...\n",
              "Name: summary, dtype: object"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Suppose you want to decode later"
      ],
      "metadata": {
        "id": "OnaLFMhfk1u_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['label'].cat.categories\n",
        "df_test['label'].cat.categories"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r0eMOMiky_8b",
        "outputId": "d0262b08-509c-4154-97af-c8397044dca5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index([0, 1, 2], dtype='int64')"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "category_map = {code: category for code, category in enumerate(df['label'].cat.categories)}\n",
        "category_map = {code: category for code, category in enumerate(df_test['label'].cat.categories)}\n",
        "category_map"
      ],
      "metadata": {
        "id": "oZEezWPak0Oh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1bcd3e2b-7667-45b8-ec48-ab4af8d28f76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 0, 1: 1, 2: 2}"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKz778AH1zSZ"
      },
      "source": [
        "### Split into train/val/test for later comparison.\n",
        "* For simplicity we split based on time.\n",
        "  - First 60% train\n",
        "  - Next 20% val\n",
        "  - Next 20% test\n",
        "* This can be problematic a bit since class balance changes over time and some articles on boundries between train/val or val/test have some overlap, but completely beats bias of stratified sample usually used since some articles are literally on same thing, but maybe different sources.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b6otP7Zi1z20",
        "outputId": "0a170645-edca-4d7d-c331-0b33c7197355"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4968, 5) (690, 5) (553, 5)\n"
          ]
        }
      ],
      "source": [
        "train_end_point = int(df.shape[0]*0.9)\n",
        "val_end_point = int(df.shape[0]*1)\n",
        "df_train = df.iloc[:train_end_point,:]\n",
        "df_val = df.iloc[train_end_point:val_end_point,:]\n",
        "# df_test = df_test.iloc[val_end_point:,:]\n",
        "print(df_train.shape, df_test.shape, df_val.shape)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert from Pandas DataFrame to Hugging Face Dataset\n",
        "* Also let's shuffle the training set.\n",
        "* We put the components train,val,test into a DatasetDict so we can access them later with HF trainer.\n",
        "* Later we will add a tokenized dataset\n"
      ],
      "metadata": {
        "id": "nv3ToinDzIwE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1g5EdzTN21Tq"
      },
      "outputs": [],
      "source": [
        "# Converting pandas DataFrames into Hugging Face Dataset objects:\n",
        "dataset_train = Dataset.from_pandas(df_train.drop('label',axis=1))\n",
        "dataset_val = Dataset.from_pandas(df_val.drop('label',axis=1))\n",
        "dataset_test = Dataset.from_pandas(df_test.drop('label',axis=1))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Shuffle the training dataset\n",
        "dataset_train_shuffled = dataset_train.shuffle(seed=42)  # Using a seed for reproducibility\n"
      ],
      "metadata": {
        "id": "EjSFUqr4zbG9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine them into a single DatasetDict\n",
        "dataset = DatasetDict({\n",
        "    'train': dataset_train_shuffled,\n",
        "    'val': dataset_val,\n",
        "    'test': dataset_test\n",
        "})\n",
        "dataset"
      ],
      "metadata": {
        "id": "xREu-St-zeGE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d39614f0-60cb-4e33-a878-88f0cd94d0f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['text1', 'text2', 'target', 'summary'],\n",
              "        num_rows: 4968\n",
              "    })\n",
              "    val: Dataset({\n",
              "        features: ['text1', 'text2', 'target', 'summary'],\n",
              "        num_rows: 553\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['text1', 'text2', 'target', 'summary'],\n",
              "        num_rows: 690\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sTP9exNizh1N",
        "outputId": "ff5b3f01-91cd-47c8-8219-c407c428657a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text1', 'text2', 'target', 'summary'],\n",
              "    num_rows: 4968\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Since our classes are not balanced let's calculate class weights based on inverse value counts\n",
        "* Convert to pytorch tensor since we will need it"
      ],
      "metadata": {
        "id": "b6mX_Hfe0hei"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_train.target.value_counts(normalize=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z6z0M7tf0g3q",
        "outputId": "a81c454b-f0d6-4e4d-d500-7b48dbe54fdd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "target\n",
              "1    0.699074\n",
              "0    0.289855\n",
              "2    0.011071\n",
              "Name: proportion, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_weights=(1/df_train.target.value_counts(normalize=True).sort_index()).tolist()\n",
        "class_weights=torch.tensor(class_weights)\n",
        "class_weights=class_weights/class_weights.sum()\n",
        "class_weights\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6Nvgfy-zsyM",
        "outputId": "e1c5c022-cc3c-4745-8ddb-67597d16eb1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.0362, 0.0150, 0.9487])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"meta-llama/Meta-Llama-3-8B\""
      ],
      "metadata": {
        "id": "Fah0aBQSuR-l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Support for third party widgets will remain active for the duration of the session. To disable support:"
      ],
      "metadata": {
        "id": "_Orej3JDvQ5r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load LLama model with 4 bit quantization as specified in bits and bytes and prepare model for peft training\n",
        "\n",
        "### Model Name"
      ],
      "metadata": {
        "id": "QolyR2Cd2Bsg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantization Config (for QLORA)"
      ],
      "metadata": {
        "id": "tzRyVNmN2jlO"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BtR7MXs43GJf"
      },
      "outputs": [],
      "source": [
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit = True, # enable 4-bit quantization\n",
        "    bnb_4bit_quant_type = 'nf4', # information theoretically optimal dtype for normally distributed weights\n",
        "    bnb_4bit_use_double_quant = True, # quantize quantized weights //insert xzibit meme\n",
        "    bnb_4bit_compute_dtype = torch.bfloat16 # optimized fp format for ML\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lora Config"
      ],
      "metadata": {
        "id": "AxRLidIwS4Xu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r = 16, # the dimension of the low-rank matrices\n",
        "    lora_alpha = 8, # scaling factor for LoRA activations vs pre-trained weight activations\n",
        "    target_modules = ['q_proj', 'k_proj', 'v_proj', 'o_proj'],\n",
        "    lora_dropout = 0.05, # dropout probability of the LoRA layers\n",
        "    bias = 'none', # wether to train bias weights, set to 'none' for attention layers\n",
        "    task_type = 'SEQ_CLS'\n",
        ")"
      ],
      "metadata": {
        "id": "EG950ljoS3RM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load model\n",
        "* AutomodelForSequenceClassification\n",
        "* Num Labels is # of classes\n"
      ],
      "metadata": {
        "id": "Brl04t2KS69t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli whoami"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efYwGgQ5v0Fx",
        "outputId": "017c1576-04b2-4514-cea8-9215675b82d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "wjfender\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 572,
          "referenced_widgets": [
            "a743916eb1364e0e85e72061f2f92931",
            "505740a29fd646d8a8c1c7f92bce71e0",
            "f4634994ad1743cdbc1bc906589fb224",
            "291cae77dbfd496e8efdf17e99d506b8",
            "d187a82d52674754a5bbc4171e38d7b9",
            "ed7e7f9566fe44b3a95faaae2551e88d",
            "3235d2a5420140888fb5c8bf8bdec008",
            "fdb9e5d0876940d59d246382d4225ce8",
            "3924acae91ba4b81bf12e560fcf9ba99",
            "79afd0fd11ac4079b0e0e8e021367e28",
            "6edbbecd9ee641d2b44e23d903e1b007"
          ]
        },
        "id": "pJtZAdKp4WdT",
        "outputId": "15d1e123-ddc6-41cc-c85e-a035e59d57b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a743916eb1364e0e85e72061f2f92931"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Meta-Llama-3-8B and are newly initialized: ['score.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LlamaForSequenceClassification(\n",
              "  (model): LlamaModel(\n",
              "    (embed_tokens): Embedding(128256, 4096)\n",
              "    (layers): ModuleList(\n",
              "      (0-31): 32 x LlamaDecoderLayer(\n",
              "        (self_attn): LlamaSdpaAttention(\n",
              "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "          (rotary_emb): LlamaRotaryEmbedding()\n",
              "        )\n",
              "        (mlp): LlamaMLP(\n",
              "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "          (act_fn): SiLU()\n",
              "        )\n",
              "        (input_layernorm): LlamaRMSNorm()\n",
              "        (post_attention_layernorm): LlamaRMSNorm()\n",
              "      )\n",
              "    )\n",
              "    (norm): LlamaRMSNorm()\n",
              "  )\n",
              "  (score): Linear(in_features=4096, out_features=3, bias=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ],
      "source": [
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=quantization_config,\n",
        "    num_labels=3,\n",
        ")\n",
        "\n",
        "model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* prepare_model_for_kbit_training() function to preprocess the quantized model for training."
      ],
      "metadata": {
        "id": "prFT0qY0mVkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = prepare_model_for_kbit_training(model)\n",
        "model"
      ],
      "metadata": {
        "id": "-NcEtG0jmTqO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cded5718-d6bc-451f-cfe6-bc3d928c3407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): LlamaForSequenceClassification(\n",
              "      (model): LlamaModel(\n",
              "        (embed_tokens): Embedding(128256, 4096)\n",
              "        (layers): ModuleList(\n",
              "          (0-31): 32 x LlamaDecoderLayer(\n",
              "            (self_attn): LlamaSdpaAttention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): LlamaMLP(\n",
              "              (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "              (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "              (act_fn): SiLU()\n",
              "            )\n",
              "            (input_layernorm): LlamaRMSNorm()\n",
              "            (post_attention_layernorm): LlamaRMSNorm()\n",
              "          )\n",
              "        )\n",
              "        (norm): LlamaRMSNorm()\n",
              "      )\n",
              "      (score): ModulesToSaveWrapper(\n",
              "        (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
              "        (modules_to_save): ModuleDict(\n",
              "          (default): Linear(in_features=4096, out_features=3, bias=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* get_peft_model prepares a model for training with a PEFT method such as LoRA by wrapping the base model and PEFT configuration with get_peft_model"
      ],
      "metadata": {
        "id": "25JDWS0Hmb0y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = get_peft_model(model, lora_config)\n",
        "model"
      ],
      "metadata": {
        "id": "zIXKJgTfmU-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "977de13b-850d-4010-e173-9e4ec7c92f61"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "PeftModelForSequenceClassification(\n",
              "  (base_model): LoraModel(\n",
              "    (model): PeftModelForSequenceClassification(\n",
              "      (base_model): LoraModel(\n",
              "        (model): LlamaForSequenceClassification(\n",
              "          (model): LlamaModel(\n",
              "            (embed_tokens): Embedding(128256, 4096)\n",
              "            (layers): ModuleList(\n",
              "              (0-31): 32 x LlamaDecoderLayer(\n",
              "                (self_attn): LlamaSdpaAttention(\n",
              "                  (q_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (k_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (v_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=1024, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (o_proj): lora.Linear4bit(\n",
              "                    (base_layer): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
              "                    (lora_dropout): ModuleDict(\n",
              "                      (default): Dropout(p=0.05, inplace=False)\n",
              "                    )\n",
              "                    (lora_A): ModuleDict(\n",
              "                      (default): Linear(in_features=4096, out_features=16, bias=False)\n",
              "                    )\n",
              "                    (lora_B): ModuleDict(\n",
              "                      (default): Linear(in_features=16, out_features=4096, bias=False)\n",
              "                    )\n",
              "                    (lora_embedding_A): ParameterDict()\n",
              "                    (lora_embedding_B): ParameterDict()\n",
              "                  )\n",
              "                  (rotary_emb): LlamaRotaryEmbedding()\n",
              "                )\n",
              "                (mlp): LlamaMLP(\n",
              "                  (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                  (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
              "                  (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
              "                  (act_fn): SiLU()\n",
              "                )\n",
              "                (input_layernorm): LlamaRMSNorm()\n",
              "                (post_attention_layernorm): LlamaRMSNorm()\n",
              "              )\n",
              "            )\n",
              "            (norm): LlamaRMSNorm()\n",
              "          )\n",
              "          (score): ModulesToSaveWrapper(\n",
              "            (original_module): Linear(in_features=4096, out_features=3, bias=False)\n",
              "            (modules_to_save): ModuleDict(\n",
              "              (default): Linear(in_features=4096, out_features=3, bias=False)\n",
              "            )\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load the tokenizer\n",
        "\n",
        "#### Since LLAMA3 pre-training doesn't have EOS token\n",
        "* Set the pad_token_id to eos_token_id\n",
        "* Set pad token ot eos_token"
      ],
      "metadata": {
        "id": "4j9Ubd2DVAOW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DzS5OhVO8Tuo",
        "outputId": "8575b4d4-03ee-4871-d509-f93ebc5a89a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, add_prefix_space=True)\n",
        "\n",
        "tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Update some model configs\n",
        "* Must use .cache = False as below or it crashes from my experience"
      ],
      "metadata": {
        "id": "akDra1649hcn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.use_cache = False\n",
        "model.config.pretraining_tp = 1"
      ],
      "metadata": {
        "id": "XBFCNrrE9hAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop through dataset to measure performance before training/fitting the model\n",
        "* Use a batch size 32 to kinda vectorize and to avoid memory errors."
      ],
      "metadata": {
        "id": "eWoLZTYf3lCw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences1 = df_test.text1.tolist()\n",
        "sentences2 = df_test.text2.tolist()\n",
        "\n",
        "sentences = list(zip(sentences1,sentences2))\n",
        "# df_test\n",
        "sentences[0:2]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "barsbGNJ08JS",
        "outputId": "0d1bc50f-8bc3-41d1-dc76-9fb62dce5387"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('It helps us identify content that might be offensive or graphic that might violate the policies of Facebook so we can flag that and review that better.',\n",
              "  \"Similarly, aside from the conversational and linguistic understanding, there's a whole thread of the work that we're doing on visual understanding.\"),\n",
              " ('It continues to drive better conversion of free trials, higher membership renewal rates for existing subscribers and higher overall engagement.',\n",
              "  \"We're going to continue to invest in video and increase that investment in 2018.\")]"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert summaries to a list\n",
        "sentences1 = df_test.text1.tolist()\n",
        "sentences2 = df_test.text2.tolist()\n",
        "\n",
        "sentences = list(zip(sentences1,sentences2))\n",
        "# Define the batch size\n",
        "batch_size = 32  # You can adjust this based on your system's memory capacity\n",
        "\n",
        "# Initialize an empty list to store the model outputs\n",
        "all_outputs = []\n",
        "\n",
        "# Process the sentences in batches\n",
        "for i in range(0, len(sentences), batch_size):\n",
        "    # Get the batch of sentences\n",
        "    batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "    # Tokenize the batch\n",
        "    inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
        "    inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
        "\n",
        "    # Perform inference and store the logits\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "        all_outputs.append(outputs['logits'])\n",
        "\n"
      ],
      "metadata": {
        "id": "1GECdZk_Iso0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Concatenate all outputs into a single tensor"
      ],
      "metadata": {
        "id": "LHBh3ML64rc6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_outputs = torch.cat(all_outputs, dim=0)\n",
        "final_outputs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0t0AWmd4kcv",
        "outputId": "29d4cd2b-4e63-4efc-c4df-7e785abe24d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.1132,  0.4743,  3.1021],\n",
              "        [ 2.5053,  0.3373,  3.4886],\n",
              "        [ 5.2064,  1.5972, -3.4047],\n",
              "        ...,\n",
              "        [-2.0321,  1.5194,  1.7328],\n",
              "        [ 5.1686,  4.6863, -2.0213],\n",
              "        [ 2.1321,  2.5148,  1.3922]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* argmax to get class prediction"
      ],
      "metadata": {
        "id": "_vfhekJH4ucy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_outputs.argmax(axis=1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K5Suw61G4qOJ",
        "outputId": "b4f54ff4-7cb2-453d-9889-0d727fb6f00c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([2, 2, 0, 1, 1, 2, 2, 1, 2, 0, 2, 2, 1, 1, 2, 2, 2, 1, 1, 2, 0, 0, 0, 2,\n",
              "        2, 0, 1, 1, 0, 2, 2, 0, 2, 2, 1, 0, 2, 2, 2, 1, 0, 2, 1, 2, 2, 2, 2, 0,\n",
              "        1, 2, 1, 0, 1, 1, 1, 2, 0, 2, 2, 2, 1, 2, 1, 1, 2, 1, 2, 1, 2, 1, 2, 0,\n",
              "        2, 1, 1, 2, 0, 2, 2, 2, 2, 2, 0, 1, 2, 1, 2, 1, 2, 1, 2, 0, 1, 2, 0, 2,\n",
              "        2, 2, 0, 0, 2, 1, 2, 1, 1, 2, 2, 2, 0, 1, 0, 2, 2, 1, 1, 1, 1, 2, 2, 2,\n",
              "        0, 1, 2, 2, 2, 2, 2, 2, 2, 0, 2, 1, 2, 2, 2, 1, 2, 0, 2, 2, 2, 2, 2, 2,\n",
              "        2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 2, 1, 2, 2, 2, 0, 1, 2, 1, 1, 1, 2, 2,\n",
              "        2, 2, 1, 0, 2, 2, 0, 1, 2, 1, 2, 1, 2, 2, 0, 1, 2, 2, 2, 2, 0, 1, 1, 1,\n",
              "        2, 2, 0, 0, 0, 2, 2, 2, 1, 0, 2, 1, 2, 2, 0, 1, 1, 1, 2, 1, 2, 1, 1, 1,\n",
              "        2, 2, 2, 1, 2, 1, 2, 1, 1, 1, 1, 2, 2, 1, 1, 2, 2, 2, 1, 1, 1, 0, 2, 0,\n",
              "        0, 2, 1, 2, 2, 2, 1, 0, 2, 2, 1, 2, 0, 2, 1, 0, 2, 1, 0, 1, 2, 2, 2, 0,\n",
              "        2, 2, 0, 2, 1, 2, 0, 2, 1, 2, 0, 2, 0, 0, 2, 1, 2, 1, 2, 1, 1, 2, 2, 2,\n",
              "        1, 2, 2, 1, 1, 2, 1, 2, 2, 1, 2, 2, 1, 0, 2, 0, 1, 2, 0, 0, 2, 0, 1, 2,\n",
              "        1, 2, 1, 1, 1, 2, 1, 0, 1, 2, 1, 2, 2, 2, 2, 1, 2, 2, 0, 2, 1, 0, 0, 2,\n",
              "        0, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 0, 2, 0, 2, 2, 0, 0, 2,\n",
              "        0, 2, 2, 2, 2, 2, 0, 1, 0, 0, 1, 2, 0, 1, 0, 1, 2, 1, 1, 2, 0, 0, 2, 1,\n",
              "        1, 1, 1, 1, 2, 0, 0, 2, 0, 2, 2, 1, 2, 2, 2, 2, 2, 1, 2, 1, 2, 1, 2, 2,\n",
              "        2, 0, 2, 2, 2, 2, 1, 2, 0, 1, 1, 1, 1, 2, 2, 2, 1, 0, 2, 1, 2, 2, 2, 0,\n",
              "        2, 2, 2, 2, 2, 2, 2, 0, 0, 2, 2, 0, 2, 2, 2, 0, 2, 1, 1, 2, 1, 2, 0, 2,\n",
              "        2, 1, 1, 2, 2, 0, 2, 2, 2, 2, 2, 2, 0, 2, 2, 0, 0, 2, 0, 2, 0, 0, 2, 1,\n",
              "        1, 2, 1, 1, 2, 2, 2, 1, 0, 1, 1, 2, 2, 1, 1, 2, 2, 0, 1, 2, 1, 0, 0, 2,\n",
              "        1, 1, 1, 2, 2, 0, 1, 2, 2, 1, 1, 0, 2, 1, 0, 1, 2, 0, 2, 2, 2, 2, 2, 1,\n",
              "        2, 1, 1, 2, 2, 2, 0, 2, 2, 2, 2, 2, 2, 2, 0, 1, 2, 0, 0, 2, 2, 2, 2, 2,\n",
              "        0, 1, 2, 2, 1, 0, 2, 0, 0, 2, 2, 2, 2, 2, 2, 1, 1, 1, 2, 0, 2, 2, 0, 0,\n",
              "        1, 2, 2, 0, 2, 2, 1, 2, 0, 0, 1, 2, 2, 2, 2, 0, 2, 1, 2, 2, 1, 1, 2, 1,\n",
              "        2, 2, 0, 2, 1, 0, 0, 2, 2, 1, 1, 0, 2, 1, 0, 2, 1, 2, 1, 2, 2, 0, 1, 2,\n",
              "        2, 1, 1, 1, 1, 2, 2, 0, 0, 0, 0, 2, 2, 2, 2, 2, 0, 2, 0, 2, 2, 2, 2, 2,\n",
              "        1, 1, 0, 2, 0, 2, 1, 1, 0, 2, 0, 2, 1, 2, 2, 2, 0, 0, 2, 1, 1, 2, 2, 2,\n",
              "        2, 1, 1, 2, 0, 1, 1, 1, 2, 1, 2, 2, 0, 2, 1, 2, 0, 1], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Move to CPU so we can use numpy and set prediction colum to it"
      ],
      "metadata": {
        "id": "mtsgVzj3JMk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
        "df_test['predictions']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XcW9bs5K5Upf",
        "outputId": "75e07599-fe2e-4918-f1b9-c6c4a8a4855f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      2\n",
              "1      2\n",
              "2      0\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "685    2\n",
              "686    1\n",
              "687    2\n",
              "688    0\n",
              "689    1\n",
              "Name: predictions, Length: 690, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['predictions'].value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-8_az8IXKyZ",
        "outputId": "21a90ed2-7545-4bd8-ae51-5299c347d6e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "predictions\n",
              "2    356\n",
              "1    200\n",
              "0    134\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 59
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use category map to get back category names"
      ],
      "metadata": {
        "id": "C4pmQIpc6RNr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_test['predictions']=df_test['predictions'].apply(lambda l:category_map[l])\n",
        "df_test['predictions']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s2mhkH5r6TgM",
        "outputId": "fd4b7d54-3ab5-4955-f542-1d7aa5c7c1e0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0      2\n",
              "1      2\n",
              "2      0\n",
              "3      1\n",
              "4      1\n",
              "      ..\n",
              "685    2\n",
              "686    1\n",
              "687    2\n",
              "688    0\n",
              "689    1\n",
              "Name: predictions, Length: 690, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analyze performance as in intro notebook"
      ],
      "metadata": {
        "id": "fPokM-op4ZZF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_performance_metrics(df_test):\n",
        "  y_test = df_test.label\n",
        "  y_pred = df_test.predictions\n",
        "\n",
        "  print(\"Confusion Matrix:\")\n",
        "  print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "  print(\"\\nClassification Report:\")\n",
        "  print(classification_report(y_test, y_pred))\n",
        "\n",
        "  print(\"Balanced Accuracy Score:\", balanced_accuracy_score(y_test, y_pred))\n",
        "  print(\"Accuracy Score:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ZJ3eFsPz4Hd9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_performance_metrics(df_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05xkpOOS03Nm",
        "outputId": "69b30ae0-9574-465b-9508-c025f4d2c2f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[ 35  54 111]\n",
            " [ 98 143 241]\n",
            " [  1   3   4]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.17      0.21       200\n",
            "           1       0.71      0.30      0.42       482\n",
            "           2       0.01      0.50      0.02         8\n",
            "\n",
            "    accuracy                           0.26       690\n",
            "   macro avg       0.33      0.32      0.22       690\n",
            "weighted avg       0.58      0.26      0.35       690\n",
            "\n",
            "Balanced Accuracy Score: 0.3238934993084371\n",
            "Accuracy Score: 0.263768115942029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trainer Components\n",
        "* model\n",
        "* tokenizer\n",
        "* training arguments\n",
        "* train dataset\n",
        "* eval dataset\n",
        "* Data Collater\n",
        "* Compute Metrics\n",
        "* class_weights: In our case since we are using a custom trainer so we can use a weighted loss we will subclass trainer and define the custom loss."
      ],
      "metadata": {
        "id": "0aM2eCK47kf2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create LLAMA tokenized dataset which will house our train/val parts during the training process but after applying tokenization"
      ],
      "metadata": {
        "id": "TQCINMPIVgvS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MAX_LEN = 512\n",
        "col_to_delete = ['text1','text2','summary']\n",
        "\n",
        "def llama_preprocessing_function(examples):\n",
        "    return tokenizer(examples['summary'], truncation=True, max_length=MAX_LEN)\n",
        "\n",
        "tokenized_datasets = dataset.map(llama_preprocessing_function, batched=True, remove_columns=col_to_delete)\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"target\", \"label\")\n",
        "tokenized_datasets.set_format(\"torch\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 113,
          "referenced_widgets": [
            "0f8e3a0e9f03406d812e0db8c1c5cc0c",
            "9e2fa510abb145f1b48a3ecfd28a2987",
            "560acb4a6b2e43c58077f7ebd872fb9f",
            "6cc8d76baadc4dbe8872940e5e62f1d2",
            "3bb5d4a13b6345658432efce613d7f0a",
            "ad6d843a51a94d3a89ec6fd3ce29d9f5",
            "c686617f2e7e478ab4fd64c49790408f",
            "202073abb28b4b8480188c9b9c742f9f",
            "7d2bec6f62884d5ead48db2136fefa65",
            "bc44345873264b5099bc15fb15b9bf71",
            "7186a3c82a2447f3a2a1f62d7f13b6c9",
            "36110a96de85443493f0148248423eb5",
            "510586fb1fc64c57b16b53a5124e4fc0",
            "957344034d954223b36cd171998dbd4f",
            "efec214ff66446dfbb16cba53e49f151",
            "b5d7d1006ee841eab11b44ee6bd788eb",
            "fc7128d22bc34b6382c3f6e0d055d7a4",
            "fac95cc2e5574297be9709d7759272ea",
            "58c7509ffe944224b315996410dfffd2",
            "9b03e7ecff124bbeaadfa27ae1197d3f",
            "c8338b937fdb4c228aaeb0fdd080d25d",
            "1b0669d126a14ce18dca00ae27c84bcc",
            "707ab14767e9480b8d5f4e0371f49c5a",
            "dcb5ea300de64eb98b8b5b898a374d2c",
            "ba9d5e9b543a459a9a1a7416fd7a9054",
            "65d49f39cb774d5199b7c54b7be7fec3",
            "e95c1299f59b458eb3a4d0741968b405",
            "f4c6191d03304e9b9fcb1a376a69944d",
            "9ca4612c427b4c89aef415eb4fa7f80a",
            "9c62bac3f3c349cabf7050c739b293a0",
            "159ddebee6e94e5486ecf7ce317fcfa1",
            "0d566ad5564e48ddba8b6e0d47c82206",
            "d2b7403092ab4cd99e139a9bb629d234"
          ]
        },
        "id": "MJ8t0ZtVVfPv",
        "outputId": "d86475c6-7be4-4c24-fa51-503a428f41f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4968 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f8e3a0e9f03406d812e0db8c1c5cc0c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/553 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "36110a96de85443493f0148248423eb5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/690 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "707ab14767e9480b8d5f4e0371f49c5a"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Collator\n",
        "A **data collator** prepares batches of data for training or inference in machine learning, ensuring uniform formatting and adherence to model input requirements. This is especially crucial for variable-sized inputs like text sequences.\n",
        "\n",
        "### Functions of Data Collator\n",
        "\n",
        "1. **Padding:** Uniformly pads sequences to the length of the longest sequence using a special token, allowing simultaneous batch processing.\n",
        "2. **Batching:** Groups individual data points into batches for efficient processing.\n",
        "3. **Handling Special Tokens:** Adds necessary special tokens to sequences.\n",
        "4. **Converting to Tensor:** Transforms data into tensors, the required format for machine learning frameworks.\n",
        "\n",
        "### `DataCollatorWithPadding`\n",
        "\n",
        "The `DataCollatorWithPadding` specifically manages padding, using a tokenizer to ensure that all sequences are padded to the same length for consistent model input.\n",
        "\n",
        "- **Syntax:** `collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)`\n",
        "- **Purpose:** Automatically pads text data to the longest sequence in a batch, crucial for models like BERT or GPT.\n",
        "- **Tokenizer:** Uses the provided `tokenizer` for sequence processing, respecting model-specific vocabulary and formatting rules.\n",
        "\n",
        "This collator is commonly used with libraries like Hugging Face's Transformers, facilitating data preprocessing for various NLP models.\n"
      ],
      "metadata": {
        "id": "LFMAsvJFVlMc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "collate_fn = DataCollatorWithPadding(tokenizer=tokenizer)\n"
      ],
      "metadata": {
        "id": "XfpRu7l5Vjx9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define which metrics to compute for evaluation\n",
        "* We will use balanced accuracy and accuracy for simplicity"
      ],
      "metadata": {
        "id": "KHh8YAiqVu06"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3fjS8YO4do1"
      },
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    predictions, labels = eval_pred\n",
        "    predictions = np.argmax(predictions, axis=1)\n",
        "    return {'balanced_accuracy' : balanced_accuracy_score(predictions, labels),'accuracy':accuracy_score(predictions,labels)}\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Define custom trainer with classweights\n",
        "* We will have a custom loss function that deals with the class weights and have class weights as additional argument in constructor"
      ],
      "metadata": {
        "id": "7IKaB5d6Wbni"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(Trainer):\n",
        "    def __init__(self, *args, class_weights=None, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        # Ensure label_weights is a tensor\n",
        "        if class_weights is not None:\n",
        "            self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n",
        "        else:\n",
        "            self.class_weights = None\n",
        "\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Extract labels and convert them to long type for cross_entropy\n",
        "        labels = inputs.pop(\"labels\").long()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        # Extract logits assuming they are directly outputted by the model\n",
        "        logits = outputs.get('logits')\n",
        "\n",
        "        # Compute custom loss with class weights for imbalanced data handling\n",
        "        if self.class_weights is not None:\n",
        "            loss = F.cross_entropy(logits, labels, weight=self.class_weights)\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits, labels)\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n"
      ],
      "metadata": {
        "id": "wc4zAX1iXvDD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# define training args"
      ],
      "metadata": {
        "id": "nfFh6JMw8y7E"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5AET29lE9qqw"
      },
      "outputs": [],
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir = 'sentiment_classification',\n",
        "    learning_rate = 1e-4,\n",
        "    per_device_train_batch_size = 8,\n",
        "    per_device_eval_batch_size = 8,\n",
        "    num_train_epochs = 2,\n",
        "    weight_decay = 0.01,\n",
        "    evaluation_strategy = 'epoch',\n",
        "    save_strategy = 'epoch',\n",
        "    load_best_model_at_end = True\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Define custom trainer"
      ],
      "metadata": {
        "id": "DyDI7Wm89-0p"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BGHhP9R09rUR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f03b4cb9-f7cc-4117-f9f9-d749d5bef82e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-66-1bea759fa04e>:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  self.class_weights = torch.tensor(class_weights, dtype=torch.float32).to(self.args.device)\n"
          ]
        }
      ],
      "source": [
        "trainer = CustomTrainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = tokenized_datasets['train'],\n",
        "    eval_dataset = tokenized_datasets['val'],\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = collate_fn,\n",
        "    compute_metrics = compute_metrics,\n",
        "    class_weights=class_weights,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* https://huggingface.co/docs/transformers/en/training"
      ],
      "metadata": {
        "id": "HCN4zFN0fRpJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Run trainer!"
      ],
      "metadata": {
        "id": "9tIQ3lxk-BLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_result = trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 262
        },
        "id": "qLXOgM0FkaKE",
        "outputId": "e7c341ad-65c4-4ce1-9db1-feaddf7dfff5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1242' max='1242' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1242/1242 1:35:42, Epoch 2/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Balanced Accuracy</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.130200</td>\n",
              "      <td>0.971857</td>\n",
              "      <td>0.676531</td>\n",
              "      <td>0.690778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.710900</td>\n",
              "      <td>0.839630</td>\n",
              "      <td>0.487608</td>\n",
              "      <td>0.772152</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:2184: UserWarning: y_pred contains classes not in y_true\n",
            "  warnings.warn(\"y_pred contains classes not in y_true\")\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:460: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Let's check the results\n",
        "* I wrapped in a function"
      ],
      "metadata": {
        "id": "dSTf1TMf8NpC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model,df_test):\n",
        "\n",
        "\n",
        "  # Convert summaries to a list\n",
        "  sentences = df_test.summary.tolist()\n",
        "\n",
        "  # Define the batch size\n",
        "  batch_size = 32  # You can adjust this based on your system's memory capacity\n",
        "\n",
        "  # Initialize an empty list to store the model outputs\n",
        "  all_outputs = []\n",
        "\n",
        "  # Process the sentences in batches\n",
        "  for i in range(0, len(sentences), batch_size):\n",
        "      # Get the batch of sentences\n",
        "      batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "      # Tokenize the batch\n",
        "      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "      # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
        "      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
        "\n",
        "      # Perform inference and store the logits\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**inputs)\n",
        "          all_outputs.append(outputs['logits'])\n",
        "  final_outputs = torch.cat(all_outputs, dim=0)\n",
        "  df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
        "  df_test['predictions']=df_test['predictions'].apply(lambda l:category_map[l])\n",
        "\n",
        "\n",
        "make_predictions(model,df_test)"
      ],
      "metadata": {
        "id": "uxIXBpIHeWKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_predictions(model,df_test):\n",
        "\n",
        "\n",
        "  # Convert summaries to a list\n",
        "  sentences = df_test.summary.tolist()\n",
        "\n",
        "  # Define the batch size\n",
        "  batch_size = 32  # You can adjust this based on your system's memory capacity\n",
        "\n",
        "  # Initialize an empty list to store the model outputs\n",
        "  all_outputs = []\n",
        "\n",
        "  # Process the sentences in batches\n",
        "  for i in range(0, len(sentences), batch_size):\n",
        "      # Get the batch of sentences\n",
        "      batch_sentences = sentences[i:i + batch_size]\n",
        "\n",
        "      # Tokenize the batch\n",
        "      inputs = tokenizer(batch_sentences, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "      # Move tensors to the device where the model is (e.g., GPU or CPU)\n",
        "      inputs = {k: v.to('cuda' if torch.cuda.is_available() else 'cpu') for k, v in inputs.items()}\n",
        "\n",
        "      # Perform inference and store the logits\n",
        "      with torch.no_grad():\n",
        "          outputs = model(**inputs)\n",
        "          all_outputs.append(outputs['logits'])\n",
        "  final_outputs = torch.cat(all_outputs, dim=0)\n",
        "  df_test['predictions']=final_outputs.argmax(axis=1).cpu().numpy()\n",
        "  df_test['predictions']=df_test['predictions'].apply(lambda l:category_map[l])\n",
        "\n",
        "\n",
        "make_predictions(model,df_test)"
      ],
      "metadata": {
        "id": "5OGdmf228OJa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_performance_metrics(df_test)"
      ],
      "metadata": {
        "id": "hrPwJ-RSsTXd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "081094a7-de53-43a1-c846-c68890a1ae29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion Matrix:\n",
            "[[137  62   1]\n",
            " [ 92 390   0]\n",
            " [  4   4   0]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.59      0.69      0.63       200\n",
            "           1       0.86      0.81      0.83       482\n",
            "           2       0.00      0.00      0.00         8\n",
            "\n",
            "    accuracy                           0.76       690\n",
            "   macro avg       0.48      0.50      0.49       690\n",
            "weighted avg       0.77      0.76      0.76       690\n",
            "\n",
            "Balanced Accuracy Score: 0.4980428769017981\n",
            "Accuracy Score: 0.763768115942029\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Saving the model trainer state and model adapters"
      ],
      "metadata": {
        "id": "eZTHqTuPir_k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "metrics = train_result.metrics\n",
        "max_train_samples = len(dataset_train)\n",
        "metrics[\"train_samples\"] = min(max_train_samples, len(dataset_train))\n",
        "trainer.log_metrics(\"train\", metrics)\n",
        "trainer.save_metrics(\"train\", metrics)\n",
        "trainer.save_state()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KnCfi0Z3W567",
        "outputId": "ffb49d55-7521-4584-d96f-1b037c93b25b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "***** train metrics *****\n",
            "  epoch                    =        2.0\n",
            "  total_flos               = 34346569GF\n",
            "  train_loss               =     0.8639\n",
            "  train_runtime            = 1:35:48.91\n",
            "  train_samples            =       4968\n",
            "  train_samples_per_second =      1.728\n",
            "  train_steps_per_second   =      0.216\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Saving the adapter model\n",
        "* Note this doesn't save the entire model. It only saves the adapters."
      ],
      "metadata": {
        "id": "CArcZkf509Xq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(\"saved_model\")"
      ],
      "metadata": {
        "id": "AyDo4GzKaQTo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference from Saved Model"
      ],
      "metadata": {
        "id": "E4JoRFNQbjxi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5JLs0pfabl48",
        "outputId": "554ce19f-5160-4305-c509-a073c5277f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r sentiment_classification_task2 /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "x7HaMw0xulhd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e112529b-eb54-4f27-fc63-5487e980dfcc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat 'sentiment_classification_task1': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp -r saved_model /content/drive/MyDrive/"
      ],
      "metadata": {
        "id": "SU3i00QSuzDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "60My1GU6u-H2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a743916eb1364e0e85e72061f2f92931": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_505740a29fd646d8a8c1c7f92bce71e0",
              "IPY_MODEL_f4634994ad1743cdbc1bc906589fb224",
              "IPY_MODEL_291cae77dbfd496e8efdf17e99d506b8"
            ],
            "layout": "IPY_MODEL_d187a82d52674754a5bbc4171e38d7b9"
          }
        },
        "505740a29fd646d8a8c1c7f92bce71e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ed7e7f9566fe44b3a95faaae2551e88d",
            "placeholder": "​",
            "style": "IPY_MODEL_3235d2a5420140888fb5c8bf8bdec008",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "f4634994ad1743cdbc1bc906589fb224": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fdb9e5d0876940d59d246382d4225ce8",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3924acae91ba4b81bf12e560fcf9ba99",
            "value": 4
          }
        },
        "291cae77dbfd496e8efdf17e99d506b8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_79afd0fd11ac4079b0e0e8e021367e28",
            "placeholder": "​",
            "style": "IPY_MODEL_6edbbecd9ee641d2b44e23d903e1b007",
            "value": " 4/4 [00:10&lt;00:00,  2.24s/it]"
          }
        },
        "d187a82d52674754a5bbc4171e38d7b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ed7e7f9566fe44b3a95faaae2551e88d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3235d2a5420140888fb5c8bf8bdec008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fdb9e5d0876940d59d246382d4225ce8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3924acae91ba4b81bf12e560fcf9ba99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "79afd0fd11ac4079b0e0e8e021367e28": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6edbbecd9ee641d2b44e23d903e1b007": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f8e3a0e9f03406d812e0db8c1c5cc0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9e2fa510abb145f1b48a3ecfd28a2987",
              "IPY_MODEL_560acb4a6b2e43c58077f7ebd872fb9f",
              "IPY_MODEL_6cc8d76baadc4dbe8872940e5e62f1d2"
            ],
            "layout": "IPY_MODEL_3bb5d4a13b6345658432efce613d7f0a"
          }
        },
        "9e2fa510abb145f1b48a3ecfd28a2987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad6d843a51a94d3a89ec6fd3ce29d9f5",
            "placeholder": "​",
            "style": "IPY_MODEL_c686617f2e7e478ab4fd64c49790408f",
            "value": "Map: 100%"
          }
        },
        "560acb4a6b2e43c58077f7ebd872fb9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_202073abb28b4b8480188c9b9c742f9f",
            "max": 4968,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7d2bec6f62884d5ead48db2136fefa65",
            "value": 4968
          }
        },
        "6cc8d76baadc4dbe8872940e5e62f1d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc44345873264b5099bc15fb15b9bf71",
            "placeholder": "​",
            "style": "IPY_MODEL_7186a3c82a2447f3a2a1f62d7f13b6c9",
            "value": " 4968/4968 [00:00&lt;00:00, 15021.58 examples/s]"
          }
        },
        "3bb5d4a13b6345658432efce613d7f0a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad6d843a51a94d3a89ec6fd3ce29d9f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c686617f2e7e478ab4fd64c49790408f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "202073abb28b4b8480188c9b9c742f9f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d2bec6f62884d5ead48db2136fefa65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc44345873264b5099bc15fb15b9bf71": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7186a3c82a2447f3a2a1f62d7f13b6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "36110a96de85443493f0148248423eb5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_510586fb1fc64c57b16b53a5124e4fc0",
              "IPY_MODEL_957344034d954223b36cd171998dbd4f",
              "IPY_MODEL_efec214ff66446dfbb16cba53e49f151"
            ],
            "layout": "IPY_MODEL_b5d7d1006ee841eab11b44ee6bd788eb"
          }
        },
        "510586fb1fc64c57b16b53a5124e4fc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc7128d22bc34b6382c3f6e0d055d7a4",
            "placeholder": "​",
            "style": "IPY_MODEL_fac95cc2e5574297be9709d7759272ea",
            "value": "Map: 100%"
          }
        },
        "957344034d954223b36cd171998dbd4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_58c7509ffe944224b315996410dfffd2",
            "max": 553,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9b03e7ecff124bbeaadfa27ae1197d3f",
            "value": 553
          }
        },
        "efec214ff66446dfbb16cba53e49f151": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c8338b937fdb4c228aaeb0fdd080d25d",
            "placeholder": "​",
            "style": "IPY_MODEL_1b0669d126a14ce18dca00ae27c84bcc",
            "value": " 553/553 [00:00&lt;00:00, 12440.33 examples/s]"
          }
        },
        "b5d7d1006ee841eab11b44ee6bd788eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc7128d22bc34b6382c3f6e0d055d7a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fac95cc2e5574297be9709d7759272ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "58c7509ffe944224b315996410dfffd2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b03e7ecff124bbeaadfa27ae1197d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c8338b937fdb4c228aaeb0fdd080d25d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1b0669d126a14ce18dca00ae27c84bcc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "707ab14767e9480b8d5f4e0371f49c5a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dcb5ea300de64eb98b8b5b898a374d2c",
              "IPY_MODEL_ba9d5e9b543a459a9a1a7416fd7a9054",
              "IPY_MODEL_65d49f39cb774d5199b7c54b7be7fec3"
            ],
            "layout": "IPY_MODEL_e95c1299f59b458eb3a4d0741968b405"
          }
        },
        "dcb5ea300de64eb98b8b5b898a374d2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f4c6191d03304e9b9fcb1a376a69944d",
            "placeholder": "​",
            "style": "IPY_MODEL_9ca4612c427b4c89aef415eb4fa7f80a",
            "value": "Map: 100%"
          }
        },
        "ba9d5e9b543a459a9a1a7416fd7a9054": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9c62bac3f3c349cabf7050c739b293a0",
            "max": 690,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_159ddebee6e94e5486ecf7ce317fcfa1",
            "value": 690
          }
        },
        "65d49f39cb774d5199b7c54b7be7fec3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0d566ad5564e48ddba8b6e0d47c82206",
            "placeholder": "​",
            "style": "IPY_MODEL_d2b7403092ab4cd99e139a9bb629d234",
            "value": " 690/690 [00:00&lt;00:00, 12808.23 examples/s]"
          }
        },
        "e95c1299f59b458eb3a4d0741968b405": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f4c6191d03304e9b9fcb1a376a69944d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9ca4612c427b4c89aef415eb4fa7f80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9c62bac3f3c349cabf7050c739b293a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159ddebee6e94e5486ecf7ce317fcfa1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "0d566ad5564e48ddba8b6e0d47c82206": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d2b7403092ab4cd99e139a9bb629d234": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}